{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# STEP 1: Bayesian Network Structure Learning\n",
    "\n",
    " \n",
    "Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
    "\n",
    "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    " - score-based structure learning\n",
    " - constraint-based structure learning\n",
    " - hybrid structure learning \n",
    " \n",
    "\n",
    "### Score-based Structure Learning\n",
    "\n",
    "\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "- A _scoring function_ maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "- A _search strategy_ to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "\n",
    "#### Scoring functions \n",
    "Resource: http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf\n",
    "\n",
    "Commonly used score to measure the fit between model and data is K2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/abhis/AppData/Local/Programs/Python/Python37-32/Lib/site-packages')\n",
    "\n",
    "from pgmpy.estimators import K2Score\n",
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "# create random data sample with 3 variables, where Z is dependent on X, Y:\n",
    "data = pd.DataFrame(np.random.randint(0, 4, size=(5000, 2)), columns=list('XY'))\n",
    "data['Z'] = data['X'] + data['Y']\n",
    "\n",
    "k2 = K2Score(data)\n",
    "\n",
    "model1 = BayesianModel([('X', 'Z'), ('Y', 'Z')])  # X -> Z <- Y\n",
    "model2 = BayesianModel([('X', 'Z'), ('X', 'Y')])  # Y <- X -> Z\n",
    "\n",
    "\n",
    "print('Model 1 K2 Score: ' + str(k2.score(model1)))\n",
    "print('Model 2 K2 Score: ' + str(k2.score(model2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the scores vary slightly, we can see that the correct `model1` has a much higher score than `model2`.\n",
    "Importantly, these scores _decompose_, i.e. they can be computed locally for each of the variables given their potential parents, independent of other parts of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k2.local_score('Z', parents=['X', 'Y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search strategies\n",
    "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n",
    "\n",
    "If only few nodes are involved (read: less than 5), ExhaustiveSearch can be used to compute the score for every DAG and returns the best-scoring one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ExhaustiveSearch\n",
    "\n",
    "es = ExhaustiveSearch(data, scoring_method=k2)\n",
    "best_model = es.estimate()\n",
    "print(best_model.edges())\n",
    "\n",
    "print(\"\\nAll DAGs by score:\")\n",
    "for score, dag in reversed(es.all_scores()):\n",
    "    print(score, dag.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more nodes are involved, one needs to switch to heuristic search. `HillClimbSearch` implements a greedy local search that starts from the DAG `start` (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# create some data with dependencies\n",
    "data = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH'))\n",
    "data['A'] += data['B'] + data['C']\n",
    "data['H'] = data['G'] - data['A']\n",
    "\n",
    "hc = HillClimbSearch(data, scoring_method=K2Score(data))\n",
    "best_model = hc.estimate()\n",
    "print(best_model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search correctly identifies e.g. that `B` and `C` do not influnce `H` directly, only through `A` and of course that `D`, `E`, `F` are independent.\n",
    "\n",
    "\n",
    "Additionally a `white_list` or `black_list` can be supplied to restrict the search to a particular subset or to exclude certain edges. The parameter `max_indegree` allows to restrict the maximum number of parents for each node.\n",
    "\n",
    "Other Commonly used scores to measure the fit between model and data are _Bayesian Dirichlet scores_ such as *BDeu* and the _Bayesian Information Criterion_ (BIC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint-based Structure Learning\n",
    "\n",
    "A different, but quite straightforward approach to build a DAG from data is this:\n",
    "\n",
    "1. Identify independencies in the data set using hypothesis tests \n",
    "2. Construct DAG (pattern) according to identified independencies\n",
    "\n",
    "#### (Conditional) Independence Tests\n",
    "\n",
    "Independencies in the data can be identified using chi2 conditional independence tests. To this end, constraint-based estimators in pgmpy have a `test_conditional_independence(X, Y, Zs)`-method, that performs a hypothesis test on the data sample. It allows to check if `X` is independent from `Y` given a set of variables `Zs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "\n",
    "data = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH'))\n",
    "data['A'] += data['B'] + data['C']\n",
    "data['H'] = data['G'] - data['A']\n",
    "data['E'] *= data['F']\n",
    "\n",
    "est = ConstraintBasedEstimator(data)\n",
    "\n",
    "print(est.test_conditional_independence('B', 'H'))          # dependent\n",
    "print(est.test_conditional_independence('B', 'E'))          # independent\n",
    "print(est.test_conditional_independence('B', 'H', ['A']))   # independent\n",
    "print(est.test_conditional_independence('A', 'G'))          # independent\n",
    "print(est.test_conditional_independence('A', 'G',  ['H']))  # dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_conditional_independence()` returns a tripel `(chi2, p_value, sufficient_data)`, consisting in the computed chi2 test statistic, the `p_value` of the test, and a heuristig flag that indicates if the sample size was sufficient. The `p_value` is the probability of observing the computed chi2 statistic (or an even higher chi2 value), given the null hypothesis that X and Y are independent given Zs.\n",
    "\n",
    "This can be used to make independence judgements, at a given level of significance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_independent(X, Y, Zs=[], significance_level=0.05):\n",
    "    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level\n",
    "\n",
    "print(is_independent('B', 'H'))\n",
    "print(is_independent('B', 'E'))\n",
    "print(is_independent('B', 'H', ['A']))\n",
    "print(is_independent('A', 'G'))\n",
    "print(is_independent('A', 'G', ['H']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAG (pattern) construction\n",
    "\n",
    "With a method for independence testing at hand, we can construct a DAG from the data set using esitmate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est.estimate(significance_level=0.01).edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Structure Learning\n",
    "\n",
    "The MMHC algorithm combines the constraint-based and score-based method. It has two parts:\n",
    "\n",
    "1. Learn undirected graph skeleton using the constraint-based construction procedure using independencies\n",
    "2. Orient edges using score-based optimization (K2 score + modified hill-climbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Bayesian Network Creation\n",
    "\n",
    "There are various standard file formats for representing PGM data. PGM data basically consists of graph, a distribution assoicated to each node and a few other attributes of a graph.\n",
    "\n",
    "`pgmpy` has a functionality to read networks from and write networks to these standard file formats. Currently pgmpy supports 5 file formats:\n",
    "- ProbModelXML \n",
    "- PomDPX \n",
    "- XMLBIF\n",
    "- XMLBeliefNetwork\n",
    "- UAI \n",
    "\n",
    "Using these modules, models can be specified in a uniform file format and readily converted to bayesian or markov model objects.\n",
    "\n",
    "Now, Let's read a ProbModel XML File and get the corresponding model instance of the probmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.readwrite import ProbModelXMLReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_string = ProbModelXMLReader('networks/example.pgmx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reader_string.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpds = model.get_cpds()\n",
    "for cpd in cpds:\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pgmpy not only allows us to read from the specific file format but also helps us to write the given model into the specific file format.\n",
    "Let's write a sample model into Probmodel XML file.\n",
    "\n",
    "For that first define our data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "edges_list = [('VisitToAsia', 'Tuberculosis'),\n",
    "              ('LungCancer', 'TuberculosisOrCancer'),\n",
    "              ('Smoker', 'LungCancer'),\n",
    "              ('Smoker', 'Bronchitis'),\n",
    "              ('Tuberculosis', 'TuberculosisOrCancer'),\n",
    "              ('Bronchitis', 'Dyspnea'),\n",
    "              ('TuberculosisOrCancer', 'Dyspnea'),\n",
    "              ('TuberculosisOrCancer', 'X-ray')]\n",
    "nodes = {'Smoker': {'States': {'no': {}, 'yes': {}},\n",
    "                    'role': 'chance',\n",
    "                    'type': 'finiteStates',\n",
    "                    'Coordinates': {'y': '52', 'x': '568'},\n",
    "                    'AdditionalProperties': {'Title': 'S', 'Relevance': '7.0'}},\n",
    "         'Bronchitis': {'States': {'no': {}, 'yes': {}},\n",
    "                        'role': 'chance',\n",
    "                        'type': 'finiteStates',\n",
    "                        'Coordinates': {'y': '181', 'x': '698'},\n",
    "                        'AdditionalProperties': {'Title': 'B', 'Relevance': '7.0'}},\n",
    "         'VisitToAsia': {'States': {'no': {}, 'yes': {}},\n",
    "                         'role': 'chance',\n",
    "                         'type': 'finiteStates',\n",
    "                         'Coordinates': {'y': '58', 'x': '290'},\n",
    "                         'AdditionalProperties': {'Title': 'A', 'Relevance': '7.0'}},\n",
    "         'Tuberculosis': {'States': {'no': {}, 'yes': {}},\n",
    "                          'role': 'chance',\n",
    "                          'type': 'finiteStates',\n",
    "                          'Coordinates': {'y': '150', 'x': '201'},\n",
    "                          'AdditionalProperties': {'Title': 'T', 'Relevance': '7.0'}},\n",
    "         'X-ray': {'States': {'no': {}, 'yes': {}},\n",
    "                   'role': 'chance',\n",
    "                   'AdditionalProperties': {'Title': 'X', 'Relevance': '7.0'},\n",
    "                   'Coordinates': {'y': '322', 'x': '252'},\n",
    "                   'Comment': 'Indica si el test de rayos X ha sido positivo',\n",
    "                   'type': 'finiteStates'},\n",
    "         'Dyspnea': {'States': {'no': {}, 'yes': {}},\n",
    "                     'role': 'chance',\n",
    "                     'type': 'finiteStates',\n",
    "                     'Coordinates': {'y': '321', 'x': '533'},\n",
    "                     'AdditionalProperties': {'Title': 'D', 'Relevance': '7.0'}},\n",
    "         'TuberculosisOrCancer': {'States': {'no': {}, 'yes': {}},\n",
    "                                  'role': 'chance',\n",
    "                                  'type': 'finiteStates',\n",
    "                                  'Coordinates': {'y': '238', 'x': '336'},\n",
    "                                  'AdditionalProperties': {'Title': 'E', 'Relevance': '7.0'}},\n",
    "         'LungCancer': {'States': {'no': {}, 'yes': {}},\n",
    "                        'role': 'chance',\n",
    "                        'type': 'finiteStates',\n",
    "                        'Coordinates': {'y': '152', 'x': '421'},\n",
    "                        'AdditionalProperties': {'Title': 'L', 'Relevance': '7.0'}}}\n",
    "edges = {'LungCancer': {'TuberculosisOrCancer': {'directed': 'true'}},\n",
    "         'Smoker': {'LungCancer': {'directed': 'true'},\n",
    "                    'Bronchitis': {'directed': 'true'}},\n",
    "         'Dyspnea': {},\n",
    "         'X-ray': {},\n",
    "         'VisitToAsia': {'Tuberculosis': {'directed': 'true'}},\n",
    "         'TuberculosisOrCancer': {'X-ray': {'directed': 'true'},\n",
    "                                  'Dyspnea': {'directed': 'true'}},\n",
    "         'Bronchitis': {'Dyspnea': {'directed': 'true'}},\n",
    "         'Tuberculosis': {'TuberculosisOrCancer': {'directed': 'true'}}}\n",
    "\n",
    "cpds = [{'Values': np.array([[0.95, 0.05], [0.02, 0.98]]),\n",
    "         'Variables': {'X-ray': ['TuberculosisOrCancer']}},\n",
    "        {'Values': np.array([[0.7, 0.3], [0.4,  0.6]]),\n",
    "         'Variables': {'Bronchitis': ['Smoker']}},\n",
    "        {'Values':  np.array([[0.9, 0.1,  0.3,  0.7], [0.2,  0.8,  0.1,  0.9]]),\n",
    "         'Variables': {'Dyspnea': ['TuberculosisOrCancer', 'Bronchitis']}},\n",
    "        {'Values': np.array([[0.99], [0.01]]),\n",
    "         'Variables': {'VisitToAsia': []}},\n",
    "        {'Values': np.array([[0.5], [0.5]]),\n",
    "         'Variables': {'Smoker': []}},\n",
    "        {'Values': np.array([[0.99, 0.01], [0.9, 0.1]]),\n",
    "         'Variables': {'LungCancer': ['Smoker']}},\n",
    "        {'Values': np.array([[0.99, 0.01], [0.95, 0.05]]),\n",
    "         'Variables': {'Tuberculosis': ['VisitToAsia']}},\n",
    "        {'Values': np.array([[1, 0, 0, 1], [0, 1, 0, 1]]),\n",
    "         'Variables': {'TuberculosisOrCancer': ['LungCancer', 'Tuberculosis']}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "model = BayesianModel(edges_list)\n",
    "\n",
    "for node in nodes:\n",
    "    model.node[node] = nodes[node]\n",
    "for edge in edges:\n",
    "    model.edge[edge] = edges[edge]\n",
    "\n",
    "tabular_cpds = []\n",
    "for cpd in cpds:\n",
    "    var = list(cpd['Variables'].keys())[0]\n",
    "    evidence = cpd['Variables'][var]\n",
    "    values = cpd['Values']\n",
    "    states = len(nodes[var]['States'])\n",
    "    evidence_card = [len(nodes[evidence_var]['States'])\n",
    "                     for evidence_var in evidence]\n",
    "    tabular_cpds.append(\n",
    "        TabularCPD(var, states, values, evidence, evidence_card))\n",
    "\n",
    "model.add_cpds(*tabular_cpds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.readwrite import ProbModelXMLWriter, get_probmodel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = get_probmodel_data(model)\n",
    "writer = ProbModelXMLWriter(model_data=model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.write_file('probmodelxml.pgmx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Bayesian Network Parameter Learning\n",
    "\n",
    "Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    "\n",
    " - Parameter learning for *discrete* nodes:\n",
    "   - Maximum Likelihood Estimation\n",
    "   - Bayesian Estimation\n",
    "   \n",
    "#### Parameter learning for *discrete* nodes\n",
    " Suppose we have this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(data={'fruit': [\"banana\", \"apple\", \"banana\", \"apple\", \"banana\",\"apple\", \"banana\", \n",
    "                                    \"apple\", \"apple\", \"apple\", \"banana\", \"banana\", \"apple\", \"banana\",], \n",
    "                          'tasty': [\"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \n",
    "                                    \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\"], \n",
    "                          'size': [\"large\", \"large\", \"large\", \"small\", \"large\", \"large\", \"large\",\n",
    "                                    \"small\", \"large\", \"large\", \"large\", \"large\", \"small\", \"small\"]})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "model = BayesianModel([('fruit', 'tasty'), ('size', 'tasty')])  # fruit -> tasty <- size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter learning is the task to estimate the values of the conditional probability distributions (CPDs), for the variables `fruit`, `size`, and `tasty`. \n",
    "\n",
    "#### State counts\n",
    "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e. for seperately for each parent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import ParameterEstimator\n",
    "pe = ParameterEstimator(model, data)\n",
    "print(\"\\n\", pe.state_counts('fruit'))  # unconditional\n",
    "print(\"\\n\", pe.state_counts('tasty'))  # conditional on fruit and size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, for example, that as many apples as bananas were observed and that `5` large bananas were tasty, while only `1` was not.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "A natural estimate for the CPDs is to simply use the *relative frequencies*, with which the variable states have occured. We observed `7 apples` among a total of `14 fruits`, so we might guess that about `50%` of `fruits` are `apples`.\n",
    "\n",
    "This approach is *Maximum Likelihood Estimation (MLE)*. According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}|\\text{model})$ is maximal. This is achieved when using the *relative frequencies*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "mle = MaximumLikelihoodEstimator(model, data)\n",
    "print(mle.estimate_cpd('fruit'))  # unconditional\n",
    "print(mle.estimate_cpd('tasty'))  # conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While very straightforward, the ML estimator has the problem of *overfitting* to the data. In above CPD, the probability of a large banana being tasty is estimated at `0.833`, because `5` out of `6` observed large bananas were tasty. Fine. But note that the probability of a small banana being tasty is estimated at `0.0`, because we  observed only one small banana and it happened to be not tasty. But that should hardly make us certain that small bananas aren't tasty!\n",
    "We simply do not have enough observations to rely on the observed frequencies. If the observed data is not representative for the underlying distribution, ML estimations will be extremly far off. \n",
    "\n",
    "When estimating parameters for Bayesian networks, lack of data is a frequent problem. Even if the total sample size is very large, the fact that state counts are done conditionally for each parents configuration causes immense fragmentation. If a variable has 3 parents that can each take 10 states, then state counts will be done seperately for `10^3 = 1000` parents configurations. This makes MLE very fragile and unstable for learning Bayesian Network parameters. A way to mitigate MLE's overfitting is *Bayesian Parameter Estimation*.\n",
    "\n",
    "#### Bayesian Parameter Estimation\n",
    "\n",
    "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables *before* the data was observed. Those \"priors\" are then updated, using the state counts from the observed data.\n",
    "\n",
    "One can think of the priors as consisting in *pseudo state counts*, that are added to the actual counts before normalization.\n",
    "Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. ones that deem all states equiprobable.\n",
    "\n",
    "A very simple prior is the so-called *K2* prior, which simply adds `1` to the count of every single state.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "est = BayesianEstimator(model, data)\n",
    "\n",
    "print(est.estimate_cpd('tasty', prior_type='K2', equivalent_sample_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated values in the CPDs are now more conservative. In particular, the estimate for a small banana being not tasty is now around `0.64` rather than `1.0`. Setting `equivalent_sample_size` to `10` means that for each parent configuration, we add the equivalent of 10 uniform samples (here: `+5` small bananas that are tasty and `+5` that aren't).\n",
    "\n",
    "`BayesianEstimator`, too, can be used via the `fit()`-method. Full example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter learning for *continuous* nodes\n",
    "\n",
    "In many situations, some variables are best modeled as taking values in some continuous space. Examples include variables such as position, velocity, temperature, and pressure. Clearly, we cannot use a table representation in this case.\n",
    "\n",
    "Nothing in the formulation of a Bayesian network requires that we restrict attention to discrete variables. The only requirement is that the CPD, P(X | Y1, Y2, ... Yn) represent, for every assignment of values y1 ∈ Val(Y1), y2 ∈ Val(Y2), .....yn ∈ val(Yn), a distribution over X. In this case, X might be continuous, in which case the CPD would need to represent distributions over a continuum of values; we might also have X’s parents continuous, so that the CPD would also need to represent a continuum of different probability distributions. There exists implicit representations for CPDs of this type, allowing us to apply all the network machinery for the continuous case as well.\n",
    "\n",
    " - Parameter learning for *continuous* nodes:\n",
    "   - Base Class for Continuous Factors\n",
    "   - Joint Gaussian Distributions\n",
    "   - Canonical Factors\n",
    "   - Linear Gaussian CPD\n",
    "   \n",
    "##### Joint Gaussian Distributions\n",
    "\n",
    "In its most common representation, a multivariate Gaussian distribution over X1………..Xn is characterized by an n-dimensional mean vector μ, and a symmetric n x n covariance matrix Σ. The density function is most defined as -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x) = \\dfrac{1}{(2\\pi)^{n/2}|Σ|^{1/2}} exp[-0.5*(x-μ)^TΣ^{-1}(x-μ)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class pgmpy.JointGaussianDistribution provides its representation. This is derived from the class pgmpy.ContinuousFactor. We need to specify the variable names, a mean vector and a covariance matrix for its inialization. It will automatically comute the pdf function given these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.factors.distributions import GaussianDistribution as JGD\n",
    "dis = JGD(['x1', 'x2', 'x3'], np.array([[1], [-3], [4]]),\n",
    "          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))\n",
    "dis.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.pdf([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/2/student_full_param.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "# Defining the model structure. We can define the network by just passing a list of edges.\n",
    "model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "# Defining individual CPDs.\n",
    "cpd_d = TabularCPD(variable='D', variable_card=2, values=[[0.6, 0.4]])\n",
    "cpd_i = TabularCPD(variable='I', variable_card=2, values=[[0.7, 0.3]])\n",
    "\n",
    "# The representation of CPD in pgmpy is a bit different than the CPD shown in the above picture. In pgmpy the colums\n",
    "# are the evidences and rows are the states of the variable. So the grade CPD is represented like this:\n",
    "#\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | diff    | intel_0 | intel_0 | intel_1 | intel_1 |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | intel   | diff_0  | diff_1  | diff_0  | diff_1  |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_0 | 0.3     | 0.05    | 0.9     | 0.5     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_1 | 0.4     | 0.25    | 0.08    | 0.3     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "#    | grade_2 | 0.3     | 0.7     | 0.02    | 0.2     |\n",
    "#    +---------+---------+---------+---------+---------+\n",
    "\n",
    "cpd_g = TabularCPD(variable='G', variable_card=3, \n",
    "                   values=[[0.3, 0.05, 0.9,  0.5],\n",
    "                           [0.4, 0.25, 0.08, 0.3],\n",
    "                           [0.3, 0.7,  0.02, 0.2]],\n",
    "                  evidence=['I', 'D'],\n",
    "                  evidence_card=[2, 2])\n",
    "\n",
    "cpd_l = TabularCPD(variable='L', variable_card=2, \n",
    "                   values=[[0.1, 0.4, 0.99],\n",
    "                           [0.9, 0.6, 0.01]],\n",
    "                   evidence=['G'],\n",
    "                   evidence_card=[3])\n",
    "\n",
    "cpd_s = TabularCPD(variable='S', variable_card=2,\n",
    "                   values=[[0.95, 0.2],\n",
    "                           [0.05, 0.8]],\n",
    "                   evidence=['I'],\n",
    "                   evidence_card=[2])\n",
    "\n",
    "# Associating the CPDs with the network\n",
    "model.add_cpds(cpd_d, cpd_i, cpd_g, cpd_l, cpd_s)\n",
    "\n",
    "# check_model checks for the network structure and CPDs and verifies that the CPDs are correctly \n",
    "# defined and sum to 1.\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now call some methods on the BayesianModel object.\n",
    "model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Network representing the Joint Distribution over the variables ?\n",
    "Till now we just have been considering that the Bayesian Network can represent the Joint Distribution without any proof. Now let's see how to compute the Joint Distribution from the Bayesian Network.\n",
    "\n",
    "From the chain rule of probabiliy we know that:  \n",
    "$$ P(A, B) = P(A | B) * P(B) $$\n",
    "\n",
    "Now in this case:  \n",
    "$$ P(D, I, G, L, S) = P(L| S, G, D, I) * P(S | G, D, I) * P(G | D, I) * P(D | I) * P(I) $$\n",
    "\n",
    "Applying the local independence conditions in the above equation we will get:  \n",
    "$$ P(D, I, G, L, S) = P(L|G) * P(S|I) * P(G| D, I) * P(D) * P(I) $$\n",
    "\n",
    "From the above equation we can clearly see that the Joint Distribution over all the variables is just the product of all the CPDs in the network. Hence encoding the independencies in the Joint Distribution in a graph structure helped us in reducing the number of parameters that we need to store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Bayesian Network Inference\n",
    "\n",
    "Till now we discussed just about representing Bayesian Networks. Now let's see how we can do inference in a Bayesian Model and use it to predict values over new data points for machine learning tasks. In this section we will consider that we already have our model. We will talk about constructing the models from data in later parts of this tutorial.\n",
    "\n",
    "In inference we try to answer probability queries over the network given some other variables. So, we might want to know the probable grade of an intelligent student in a difficult class given that he scored good in SAT. So for computing these values from a Joint Distribution we will have to reduce over the given variables that is $ I = 1 $, $ D = 1 $, $ S = 1 $ and then marginalize over the other variables that is $ L $ to get $ P(G | I=1, D=1, S=1) $.\n",
    "But carrying on marginalize and reduce operation on the complete Joint Distribution is computationaly expensive since we need to iterate over the whole table for each operation and the table is exponential is size to the number of variables. But in Graphical Models we exploit the independencies to break these operations in smaller parts making it much faster.\n",
    "\n",
    "One of the very basic methods of inference in Graphical Models is __Variable Elimination__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Elimination\n",
    "We know that:\n",
    "\n",
    "$ P(D, I, G, L, S) = P(L|G) * P(S|I) * P(G|D, I) * P(D) * P(I) $\n",
    "\n",
    "Now let's say we just want to compute the probability of G. For that we will need to marginalize over all the other variables.\n",
    "\n",
    "$ P(G) = \\sum_{D, I, L, S} P(D, I, G, L, S) $\n",
    "$ P(G) = \\sum_{D, I, L, S} P(L|G) * P(S|I) * P(G|D, I) * P(D) * P(I) $\n",
    "$ P(G) = \\sum_D \\sum_I \\sum_L \\sum_S P(L|G) * P(S|I) * P(G|D, I) * P(D) * P(I) $\n",
    "\n",
    "Now since not all the conditional distributions depend on all the variables we can push the summations inside:\n",
    "\n",
    "$ P(G) = \\sum_D \\sum_I \\sum_L \\sum_S P(L|G) * P(S|I) * P(G|D, I) * P(D) * P(I) $  \n",
    "$ P(G) = \\sum_D P(D) \\sum_I P(G|D, I) * P(I) \\sum_S P(S|I) \\sum_L P(L|G) $\n",
    "\n",
    "So, by pushing the summations inside we have saved a lot of computation because we have to now iterate over much smaller tables.\n",
    "\n",
    "Let's take an example for inference using Variable Elimination in pgmpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "infer = VariableElimination(model)\n",
    "print(infer.query(['G']) ['G'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be cases in which we want to compute the conditional distribution let's say $ P(G | D=0, I=1) $. In such cases we need to modify our equations a bit:\n",
    "\n",
    "$ P(G | D=0, I=1) = \\sum_L \\sum_S P(L|G) * P(S| I=1) * P(G| D=0, I=1) * P(D=0) * P(I=1) $\n",
    "$ P(G | D=0, I=1) = P(D=0) * P(I=1) * P(G | D=0, I=1) * \\sum_L P(L | G) * \\sum_S P(S | I=1) $\n",
    "\n",
    "In pgmpy we will just need to pass an extra argument in the case of conditional distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer.query(['G'], evidence={'D': 0, 'I': 1}) ['G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/2/three_nodes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the above cases we will see the flow of influence from $ A $ to $ C $ under various cases.\n",
    "\n",
    "1. __Causal:__ In the general case when we make any changes in the variable $ A $, it will have effect of variable $ B $ (as we discussed above) and this change in $ B $ will change the values in $ C $. One other possible case can be when $ B $ is observed i.e. we know the value of $ B $. So, in this case any change in $ A $ won't affect $ B $ since we already know the value. And hence there won't be any change in $ C $ as it depends only on $ B $. Mathematically  we can say that: $ (A \\perp C | B) $.\n",
    "2. __Evidential:__ Similarly in this case also observing $ B $ renders $ C $ independent of $ A $. Otherwise when $ B $ is not observed the influence flows from $ A $ to $ C $. Hence $ (A \\perp C | B) $.\n",
    "3. __Common Evidence:__ This case is a bit different from the others. When $ B $ is not observed any change in $ A $ reflects some change in $ B $ but not in $ C $. Let's take the example of $ D \\rightarrow G \\leftarrow I $. In this case if we increase the difficulty of the course the probability of getting a higher grade reduces but this has no effect on the intelligence of the student. But when $ B $ is observed let's say that the student got a good grade. Now if we increase the difficulty of the course this will increase the probability of the student to be intelligent since we already know that he got a good grade. Hence in this case $ (A \\perp C) $ and $ ( A \\not\\perp C | B) $. This structure is also commonly known as V structure.\n",
    "4. __Common Cause:__ The influence flows from $ A $ to $ C $ when $ B $ is not observed. But when $ B $ is observed and change in $ A $ doesn't affect $ C $ since it's only dependent on $ B $. Hence here also $ ( A \\perp C | B) $. \n",
    "\n",
    "Let's not see a few examples for finding the independencies in a newtork using pgmpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
